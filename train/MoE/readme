

---

# Uncertainty-Aware Mixture of Experts (MoE) Training Pipeline

This directory contains the implementation of the **Consistency-Filtered Mixture of Experts (MoE)** architecture for Satellite-to-Street View synthesis in disaster scenarios.

Unlike standard single-model approaches (e.g., Pix2Pix or vanilla ControlNet), this framework decouples **geometric structure** from **semantic texture** (damage severity) by employing a Router-Expert mechanism. This addresses the "averaging effect" and VLM hallucinations often found in disaster generation tasks.

## ðŸ“‚ File Structure

| File Name | Description |
| --- | --- |
| **`MoE_Consistency_Pipeline.py`** | **The Master Pipeline.** Handles data cleaning, label alignment (Ground Truth vs. Gemini), consistency filtering, dataset splitting, and trains the **Router (ResNet18)**. |
| **`optimized_expert_trainer.py`** | **The Expert Trainer.** An optimized training script for ControlNet using Gradient Accumulation and xformers to enable efficient fine-tuning on consumer GPUs. |
| **`run_all_experts.py`** | **The Orchestrator.** A one-click script to sequentially train the three experts (**Mild**, **Moderate**, **Severe**) after the data and Router are prepared. |

## ðŸš€ Methodology

This pipeline implements a novel three-stage training process:

1. **Consistency Filtering (Data Engineering):**
* We align Ground Truth (GT) labels with VLM (Gemini) predictions.
* **filtering Strategy:** Only samples where `GT == Gemini` are retained to train the Router. This removes "noisy" data caused by viewpoint ambiguity (e.g., "pancaking" collapses visible from the street but not from satellite).


2. **Router Training (The "Brain"):**
* A **ResNet18** classifier is trained on the filtered "consistent" dataset.
* It learns to classify satellite imagery into *Mild*, *Moderate*, or *Severe* based purely on visual texture.


3. **Expert Training (The "Hands"):**
* Three independent **ControlNet** models are trained, each specializing in a specific damage level.
* *Mild Expert:* Learned on strictly non-damaged structures.
* *Severe Expert:* Learned on strictly destroyed structures.



## ðŸ› ï¸ Usage

### Prerequisites

Ensure you have the necessary dependencies installed:

```bash
pip install torch torchvision diffusers transformers accelerate pandas opencv-python

```

### Configuration

Before running, open `MoE_Consistency_Pipeline.py` and `run_all_experts.py` to verify your data paths:

```python
BASE_DIR = r"D:\your_path\data"  # Update this to your local data directory

```

### Step 1: Data Preparation & Router Training

Run the master pipeline to clean data, split datasets, and train the Router.

```bash
python MoE_Consistency_Pipeline.py

```

*Output:* * Processed CSVs in `moe_processed_data/`

* Trained Router model: `moe_checkpoints/router_consistent.pth`

### Step 2: Train Experts

Once Step 1 is complete, run the orchestration script to train the Mild, Moderate, and Severe experts sequentially.

```bash
python run_all_experts.py

```

*Output:*

* Trained ControlNet checkpoints in `moe_checkpoints/expert_mild`, `expert_moderate`, and `expert_severe`.

## ðŸ“Š Performance Optimization

The `optimized_expert_trainer.py` includes several memory optimizations:

* **Gradient Accumulation:** Simulates a larger batch size (e.g., Batch=4) while physically using Batch=1.
* **xformers:** Memory-efficient attention (automatically enabled if installed).
* **FP16 Mixed Precision:** Reduces VRAM usage.

## citation

If you use this code in your research, please cite:

> [Your Name/Lab Name], "Disaster-MoE: Decoupling Semantic Texture from Geometric Structure for Uncertainty-Aware Satellite-to-Street View Synthesis," 2025.
